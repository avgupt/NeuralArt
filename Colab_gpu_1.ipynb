{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Working_NST_1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMPKJ5NjptJu64672QewMHj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"O9gLD0gfo7lQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593937925827,"user_tz":-330,"elapsed":918,"user":{"displayName":"Shivangi Tomar","photoUrl":"","userId":"08073642674029743437"}},"outputId":"9496fcc1-e59b-4c18-f8f6-94183a90ac51"},"source":["%%writefile requirements.txt\n","Flask==0.12.2\n","flask-socketio\n","eventlet==0.17.4\n","gunicorn==18.0.0"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Overwriting requirements.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q6kjPYkKWPNJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593938297598,"user_tz":-330,"elapsed":691,"user":{"displayName":"Shivangi Tomar","photoUrl":"","userId":"08073642674029743437"}}},"source":[""],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"DrW6cK5tsTkH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593938412262,"user_tz":-330,"elapsed":1050,"user":{"displayName":"Shivangi Tomar","photoUrl":"","userId":"08073642674029743437"}},"outputId":"afb56b5a-6f93-4bf4-9b56-745b0fecb2fa"},"source":["%%writefile main.py\n","\n","from flask import Flask, render_template, request, send_file, url_for\n","from werkzeug.utils import secure_filename\n","\n","app = Flask(__name__, static_folder='static')\n","\n","\n","@app.route('/')\n","def to_upload_page():\n","    return render_template('to_upload.html')\n","    \n","\n","\n","def model(content, style):\n","  from PIL import Image\n","  import numpy as np\n","\n","  from keras import backend\n","  from keras.models import Model\n","  from keras.applications.vgg16 import VGG16\n","\n","  from scipy.optimize import fmin_l_bfgs_b\n","  #from scipy.misc import imsave\n","\n","  import time\n","\n","  HEIGHT = 512\n","  WIDTH = 512\n","  SHAPE = (WIDTH, HEIGHT)\n","\n","  content_img_path = 'static/' + content + '.jpg'\n","  style_img_path = 'static/' + style + '.jpg'\n","\n","  # define a func to load images\n","  def load_image(image_path):\n","      image = Image.open(image_path)\n","      image = image.resize(SHAPE)\n","      return image\n","\n","\n","  # convert images into a form suitable for numerical processing.\n","  def image_to_float(image):\n","      img_array = np.asarray(image, dtype = 'float32')\n","      img_array = np.expand_dims(img_array, axis = 0)\n","      return img_array\n","\n","  # subtract mean RGB value from each pixel and conver RGB to BGR\n","  def convert_pixels(array):\n","      array[:, :, :, 0] -= 103.939\n","      array[:, :, :, 1] -= 116.779\n","      array[:, :, :, 2] -= 123.68\n","      return array[:, :, :, ::-1]\n","      \n","  # content loss is euclidean dist bw feature representations of the content and combination loss\n","  def content_loss(content, combination):\n","      return backend.sum(backend.square(combination - content))\n","\n","  # gram mtrix is computed by reshaping the feature spaces suitably by taking their outer product\n","  def gram_matrix(x):\n","      features = backend.batch_flatten(backend.permute_dimensions(x, (2, 0, 1)))\n","      gram = backend.dot(features, backend.transpose(features))\n","      return gram\n","\n","  #---style loss---(we use gram matrix)\n","  def style_loss(style, combination):\n","      S = gram_matrix(style)\n","      C = gram_matrix(combination)\n","      channels = 3\n","      size = HEIGHT * WIDTH\n","      return backend.sum(backend.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n","\n","  # for smootness of image\n","  def total_variation_loss(x):\n","      a = backend.square(x[:, :HEIGHT - 1, :WIDTH - 1, :]- x[:, 1:, :WIDTH-1, :])\n","      b = backend.square(x[:, :HEIGHT-1, :WIDTH-1, :] - x[:, :HEIGHT-1, 1:, :])\n","      return backend.sum(backend.pow(a + b, 1.25))\n","\n","\n","  def eval_loss_and_grads(x):\n","      x = x.reshape((1, HEIGHT, WIDTH, 3))\n","      outs = f_outputs([x])\n","      loss_value = outs[0]\n","      grad_values = outs[1].flatten().astype('float64')\n","      return loss_value, grad_values\n","\n","  # Evalutor class computes loss and gradients \n","  class Evaluator(object):\n","      '''computes loss and Gradients'''\n","      def __init__(self):\n","          self.loss_value = None\n","          self.grads_value = None\n","          \n","      def loss(self, x):\n","        # assert self.loss_value is None\n","          loss_value, grad_values = eval_loss_and_grads(x)\n","          self.loss_value = loss_value\n","          self.grad_values = grad_values\n","          return self.loss_value\n","      \n","      def grads(self, x):\n","          #assert self.loss_value is None\n","          grad_values = np.copy(self.grad_values)\n","          self.loss__value = None\n","          self.grad_values = None\n","          return grad_values\n","          \n","          \n","  content_image = load_image(content_img_path)\n","  style_image = load_image(style_img_path)\n","\n","  content_image.show()\n","  style_image.show()\n","\n","\n","  content_array = image_to_float(content_image)\n","  style_array = image_to_float(style_image)\n","\n","  print(content_array.shape)\n","  print(style_array.shape)\n","\n","  content_array = convert_pixels(content_array)\n","  style_array = convert_pixels(style_array)\n","\n","  #use the arrays to define variables in keras backend\n","  content_image = backend.variable(content_array)\n","  style_image = backend.variable(style_array)\n","\n","  # introduce placeholder var to store the combination image.\n","  combination_image = backend.placeholder((1, HEIGHT, WIDTH, 3))\n","\n","  # concenrate all the img data into a single tensor that's suitable for processing by Keras's VGG16 model\n","  input_tensor = backend.concatenate([content_image, style_image, combination_image], axis = 0)\n","\n","  # defining our model\n","  model = VGG16(input_tensor = input_tensor, weights = 'imagenet', include_top = False)\n","\n","  # layers in model\n","  layers = dict([(layer.name, layer.output) for layer in model.layers])\n","\n","  # relative importance of content_loss, style_loss and total variation loss\n","  content_weight = 0.025 \n","  style_weight = 5.0\n","  total_variatio_weight = 1.0\n","\n","  # initially total loss will be 0\n","  loss = backend.variable(0.)\n","  #---content loss---\n","\n","  # we use block2_conv2 layer to get content \n","  layer_features = layers['block2_conv2']\n","  content_image_features = layer_features[0, :, :, :]\n","  combination_features = layer_features[2, :, :, :]\n","\n","  loss = content_weight * content_loss(content_image_features,\n","                                        combination_features)\n","\n","  # total style loss = style_weight per layer * style loss\n","\n","  style_feature_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3', 'block5_conv3']\n","  for layer in style_feature_layers:\n","      layer_features = layers[layer]\n","      style_features = layer_features[1, :, :, :]\n","      combination_features = layer_features[2, :, :, :]\n","      sty_los = style_loss(style_features, combination_features)\n","      loss += (style_weight / len(style_feature_layers)) * sty_los\n","      \n","\n","  # total variation loss - for optimization - for smootness\n","\n","  loss += total_variatio_weight * total_variation_loss(combination_image)\n","\n","  # gradient of the total loss relative to the combination img \n","\n","  grads = backend.gradients(loss, combination_image)\n","\n","  outputs = [loss]\n","  outputs += grads\n","  f_outputs = backend.function([combination_image], outputs)\n","\n","  evaluator = Evaluator()\n","\n","  x = np.random.uniform(0, 255, (1, HEIGHT, WIDTH, 3)) - 128\n","\n","  iterations = 10\n","\n","  for _ in range(iterations):\n","      print('Start of Iteration :-' + str(_))\n","      start_time = time.time()\n","      x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime = evaluator.grads, maxfun = 20)\n","      print('Current Loss value :' + str(min_val))\n","      end_time = time.time()\n","      print('Iteration %d completed in %ds' % (_, end_time - start_time))\n","      \n","  # subject output img to the inverse of transformation \n","      \n","  x = x.reshape((HEIGHT, WIDTH, 3))\n","  x = x[:, :, ::-1]\n","\n","  x[:, :, 0] += 103.939\n","  x[:, :, 1] += 116.779\n","  x[:, :, 2] += 123.68\n","\n","  x = np.clip(x, 0, 255).astype('uint8')\n","  import uuid\n","  transformed = str(uuid.uuid4())\n","\n","  (Image.fromarray(x)).save('static/' + transformed + '.jpg')\n","  return transformed\n","\n","\n","\n","def makevar():\n","  import uuid\n","  content = str(uuid.uuid4())\n","  style = str(uuid.uuid4())\n","  return content, style\n","\n","\n","\n","\n","@app.route('/uploader', methods=['POST'])\n","def file_upload():\n","    \n","    \n","    content, style = makevar()\n","\n","    f = request.files['contentFile']\n","    f.save('static/' + content + '.jpg')\n","    f2 = request.files['styleFile']\n","    f2.save('static/' + style + '.jpg')\n","    #import neural_model.py\n","    #import os \n","    #os.system('python3 neural_model.py')\n","    transformed = model(content, style)\n","    #from neural_model import transformed\n","    filename = transformed + '.jpg'\n","    return render_template('file_uploaded.html', filename=filename)\n","\n","\n","if __name__ == '__main__':\n","    app.run(debug=True)\n","\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Overwriting main.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lNaBq_hOpLQu","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593937937022,"user_tz":-330,"elapsed":5152,"user":{"displayName":"Shivangi Tomar","photoUrl":"","userId":"08073642674029743437"}}},"source":["%mkdir templates -p\n","%mkdir static -p"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7sjB3OlpQAB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593937937027,"user_tz":-330,"elapsed":3379,"user":{"displayName":"Shivangi Tomar","photoUrl":"","userId":"08073642674029743437"}},"outputId":"cce8e18a-4edc-47da-a15d-694074a20078"},"source":["%%writefile templates/layout.html\n","\n","<!DOCTYPE html>\n","<html>\n","    <head>\n","        <title> AppName </title>\n","    </head>\n","\n","    <body>\n","        {% block body %} {% endblock %}\n","    </body>\n","</html>"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Overwriting templates/layout.html\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d_COaoVwpTQT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593937939244,"user_tz":-330,"elapsed":865,"user":{"displayName":"Shivangi Tomar","photoUrl":"","userId":"08073642674029743437"}},"outputId":"ea1c62d9-d3b5-4c0b-ea0b-150e70b7ef80"},"source":["%%writefile templates/to_upload.html\n","\n","{% extends \"layout.html\" %}\n","\n","{% block body %}\n","<form action=\"/uploader\" method=\"POST\" enctype=\"multipart/form-data\">       \n","    Select content image to upload: \n","    <input type=\"file\" name=\"contentFile\" id=\"contentFile\">\n","    <br>\n","    Select style image to upload: \n","    <input type=\"file\" name=\"styleFile\" id=\"styleFile\">\n","    <br>\n","    <input type=\"submit\" value=\"Upload Image\" name=\"submit\">\n","    \n","</form>\n","{% endblock %}"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Overwriting templates/to_upload.html\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ae1URF3MqA4p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593937941185,"user_tz":-330,"elapsed":829,"user":{"displayName":"Shivangi Tomar","photoUrl":"","userId":"08073642674029743437"}},"outputId":"3bd6d9ee-080a-4d08-8810-301ea89d2f8e"},"source":["%%writefile templates/file_uploaded.html\n","\n","{% extends \"layout.html\" %}\n","\n","{% block body %}\n","\n","    <h1> Image Upload Successful ! </h1> \n","    \n","    <img src= \"{{ url_for('static', filename=filename) }}\" alt=\"Your pic\" width=\"500\" height=\"450\">\n","\n","{% endblock %}"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Overwriting templates/file_uploaded.html\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Az2x9Fqeqfhn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593937942925,"user_tz":-330,"elapsed":860,"user":{"displayName":"Shivangi Tomar","photoUrl":"","userId":"08073642674029743437"}}},"source":["get_ipython().system_raw(\n","    'pip3 install -r requirements.txt && python3 main.py > logs.txt 2>&1 &'\n",")"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"uyJJUaegsr8T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1593937947329,"user_tz":-330,"elapsed":3434,"user":{"displayName":"Shivangi Tomar","photoUrl":"","userId":"08073642674029743437"}},"outputId":"20dcc554-cbca-4fb1-8d52-1dab28ac90aa"},"source":["!tail logs.txt"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Iteration 7 completed in 15s\n","Start of Iteration :-8\n","Current Loss value :24628345000.0\n","Iteration 8 completed in 15s\n","Start of Iteration :-9\n","Current Loss value :24487520000.0\n","Iteration 9 completed in 15s\n"," * Restarting with stat\n"," * Debugger is active!\n"," * Debugger PIN: 268-100-759\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LMij0WhHsuEq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":422},"executionInfo":{"status":"ok","timestamp":1593937951098,"user_tz":-330,"elapsed":2646,"user":{"displayName":"Shivangi Tomar","photoUrl":"","userId":"08073642674029743437"}},"outputId":"2f675de1-c320-4d38-b6d0-579ad93cc5d4"},"source":["# Make sure it's running on local port\n","PORT = 5000\n","\n","!curl http://localhost:{PORT}/home"],"execution_count":11,"outputs":[{"output_type":"stream","text":["\n","\n","<!DOCTYPE html>\n","<html>\n","    <head>\n","        <title> AppName </title>\n","    </head>\n","\n","    <body>\n","        \n","<form action=\"/uploader\" method=\"POST\" enctype=\"multipart/form-data\">       \n","    Select content image to upload: \n","    <input type=\"file\" name=\"contentFile\" id=\"contentFile\">\n","    <br>\n","    Select style image to upload: \n","    <input type=\"file\" name=\"styleFile\" id=\"styleFile\">\n","    <br>\n","    <input type=\"submit\" value=\"Upload Image\" name=\"submit\">\n","    \n","</form>\n","\n","    </body>\n","</html>"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZKrvFeJdswuw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1593939958272,"user_tz":-330,"elapsed":6356,"user":{"displayName":"Shivangi Tomar","photoUrl":"","userId":"08073642674029743437"}},"outputId":"a6d431ab-d800-424e-965d-ea592970499e"},"source":["!wget -nc --quiet https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O ngrok-stable-linux-amd64.zip\n","!unzip -o ngrok-stable-linux-amd64.zip"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6HEwPe6usz9m","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593937966373,"user_tz":-330,"elapsed":885,"user":{"displayName":"Shivangi Tomar","photoUrl":"","userId":"08073642674029743437"}}},"source":["PORT=5000\n","\n","get_ipython().system_raw(\n","    './ngrok http {} &'\n","    .format(PORT)\n",")"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"nri8Z7UBs4V4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593937970277,"user_tz":-330,"elapsed":2651,"user":{"displayName":"Shivangi Tomar","photoUrl":"","userId":"08073642674029743437"}},"outputId":"abe69d28-28e1-4d20-df96-308b4bf5ead5"},"source":["public_url = !curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n","\n","print(public_url[0])"],"execution_count":14,"outputs":[{"output_type":"stream","text":["http://e3dbf026b8e8.ngrok.io\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EL7PK-2rKgwP","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}